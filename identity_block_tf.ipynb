{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "identity_block_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasirHabib/Deep-Learning-Advanced-Computer-Vision/blob/master/identity_block_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c87zAPGKyRR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ukgr5xXy6wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJRoJ4JwzA-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_filter(shape):\n",
        "  w = np.random.randn(*shape) * np.sqrt(2.0 / np.prod(shape[:-1]))\n",
        "  return w.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5gmF090zNCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer():\n",
        "  def __init__(self, num_feature_maps, num_color_channels, filter_width, filter_height, stride, padding):\n",
        "    sz = (filter_width, filter_height, num_color_channels, num_feature_maps)\n",
        "    W = init_filter(sz)\n",
        "    b = np.zeros(num_feature_maps, dtype=np.float32)\n",
        "\n",
        "    self.W = tf.Variable(W)\n",
        "    self.b = tf.Variable(b)\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    \n",
        "  def ConvLayer_forward(self, X):\n",
        "    conv_out = tf.nn.conv2d(X, self.W, strides=[1, self.stride, self.stride, 1], padding=self.padding)\n",
        "    conv_out = tf.nn.bias_add(conv_out, self.b)\n",
        "    return conv_out\n",
        "  \n",
        "  def copyFromKerasLayers(self, layer):\n",
        "    # only 1 layer to copy from\n",
        "    W, b = layer.get_weights()\n",
        "    op1 = self.W.assign(W)\n",
        "    op2 = self.b.assign(b)\n",
        "    self.session.run((op1, op2))\n",
        "\n",
        "  #def copyFromWeights(self, W, b):\n",
        "  # op1 = self.W.assign(W)\n",
        "  # op2 = self.b.assign(b)\n",
        "  # self.session.run((op1, op2))\n",
        "\n",
        "  def get_params(self):\n",
        "    return [self.W, self.b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Km9NlW50wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNormLayer():\n",
        "  def __init__(self, D):\n",
        "    self.running_mean = tf.Variable(np.zeros(D, dtype=np.float32), trainable=False)\n",
        "    self.running_var  = tf.Variable(np.ones(D, dtype=np.float32), trainable=False)\n",
        "    self.gamma        = tf.Variable(np.ones(D, dtype=np.float32))\n",
        "    self.beta         = tf.Variable(np.zeros(D, dtype=np.float32))\n",
        "\n",
        "  def BatchNormLayer_forward(self, X):\n",
        "    return tf.nn.batch_normalization(X, self.running_mean, self.running_var, self.beta, self.gamma, 1e-3)\n",
        "    \n",
        "  def copyFromKerasLayers(self, layer):\n",
        "    # only 1 layer to copy from\n",
        "    # order:\n",
        "    # gamma, beta, moving mean, moving variance\n",
        "    gamma, beta, running_mean, running_var = layer.get_weights()\n",
        "    op1 = self.running_mean.assign(running_mean)\n",
        "    op2 = self.running_var.assign(running_var)\n",
        "    op3 = self.gamma.assign(gamma)\n",
        "    op4 = self.beta.assign(beta)\n",
        "    self.session.run((op1, op2, op3, op4))\n",
        "\n",
        "  def get_params(self):\n",
        "    return [self.running_mean, self.running_var, self.gamma, self.beta]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz4Gw1EO6BW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IdentityBlock():\n",
        "  def __init__(self, num_color_channels, fm_sizes, activation=tf.nn.relu):\n",
        "    assert(len(fm_sizes) == 3)\n",
        "    \n",
        "    \n",
        "    # note: stride=1 applies to conv 1,2,3 in main branch\n",
        "    \n",
        "    self.session = None\n",
        "    self.activation = activation\n",
        "    \n",
        "    self.conv1 = ConvLayer(fm_sizes[0], num_color_channels, 1, 1, stride=1, padding='VALID')     # DO NOT USE SAME MODE WHEN YOU HAVE 1x1 Convolutions\n",
        "    self.bn1 = BatchNormLayer(fm_sizes[0])\n",
        "    self.conv2 = ConvLayer(fm_sizes[1], fm_sizes[0], 3, 3, stride=1, padding='SAME')\n",
        "    self.bn2 = BatchNormLayer(fm_sizes[1])\n",
        "    self.conv3 = ConvLayer(fm_sizes[2], fm_sizes[1], 1, 1, stride=1, padding='VALID')\n",
        "    self.bn3 = BatchNormLayer(fm_sizes[2])\n",
        "    \n",
        "    # in case needed later\n",
        "    self.layers = [\n",
        "        self.conv1, self.bn1,\n",
        "        self.conv2, self.bn2,\n",
        "        self.conv3, self.bn3\n",
        "    ]\n",
        "    \n",
        "    # this will not be used when input passed in from a previous layer\n",
        "    self.input_ = tf.placeholder(tf.float32, shape=(1, 224, 224, num_color_channels))\n",
        "    \n",
        "    self.output = self.tf_forward(self.input_)\n",
        "    \n",
        "  def tf_forward(self, X):\n",
        "    FX=self.conv1.ConvLayer_forward(X)\n",
        "    FX=self.bn1.BatchNormLayer_forward(FX)\n",
        "    FX=self.activation(FX)\n",
        "\n",
        "    FX=self.conv2.ConvLayer_forward(FX)\n",
        "    FX=self.bn2.BatchNormLayer_forward(FX)\n",
        "    FX=self.activation(FX)\n",
        "\n",
        "    FX=self.conv3.ConvLayer_forward(FX)\n",
        "    FX=self.bn3.BatchNormLayer_forward(FX)\n",
        "\n",
        "    return self.activation(FX+X)\n",
        "    \n",
        "  def predict(self, X):\n",
        "    assert(self.session is not None)\n",
        "    return self.session.run(self.output, feed_dict={self.input_: X})\n",
        "    \n",
        "  def set_session(self, session):\n",
        "    # need to make this a session\n",
        "    # so assignment happens on sublayers too\n",
        "    self.session = session\n",
        "    self.conv1.session = session\n",
        "    self.bn1.session = session\n",
        "    self.conv2.session = session\n",
        "    self.bn2.session = session\n",
        "    self.conv3.session = session\n",
        "    self.bn3.session = session\n",
        "      \n",
        "  def copyFromKerasLayers(self, layers):\n",
        "    assert(len(layers) == 10)\n",
        "    # <keras.layers.convolutional.Conv2D at 0x7fa44255ff28>,\n",
        "    # <keras.layers.normalization.BatchNormalization at 0x7fa44250e7b8>,\n",
        "    # <keras.layers.core.Activation at 0x7fa44252d9e8>,\n",
        "    # <keras.layers.convolutional.Conv2D at 0x7fa44253af60>,\n",
        "    # <keras.layers.normalization.BatchNormalization at 0x7fa4424e4f60>,\n",
        "    # <keras.layers.core.Activation at 0x7fa442494828>,\n",
        "    # <keras.layers.convolutional.Conv2D at 0x7fa4424a2da0>,\n",
        "    # <keras.layers.normalization.BatchNormalization at 0x7fa44244eda0>,\n",
        "    # <keras.layers.merge.Add at 0x7fa44245d5c0>,\n",
        "    # <keras.layers.core.Activation at 0x7fa44240aba8>\n",
        "    self.conv1.copyFromKerasLayers(layers[0])\n",
        "    self.bn1.copyFromKerasLayers(layers[1])\n",
        "    self.conv2.copyFromKerasLayers(layers[3])\n",
        "    self.bn2.copyFromKerasLayers(layers[4])\n",
        "    self.conv3.copyFromKerasLayers(layers[6])\n",
        "    self.bn3.copyFromKerasLayers(layers[7])\n",
        "      \n",
        "  def get_params(self):\n",
        "    params = []\n",
        "    for layer in self.layers:\n",
        "      params += layer.get_params()\n",
        "    return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmPt_61aCfN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "857a78e3-c48f-40b9-a31a-84c2e42d24ce"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  identity_block = IdentityBlock(num_color_channels=256, fm_sizes=[64, 64, 256])\n",
        "  \n",
        "  # make a fake image\n",
        "  X = np.random.random((1, 224, 224, 256))\n",
        "  \n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as session:\n",
        "    identity_block.set_session(session)\n",
        "    session.run(init)\n",
        "    \n",
        "    output = identity_block.predict(X)\n",
        "    print(\"output.shape:\", output.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output.shape: (1, 224, 224, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}